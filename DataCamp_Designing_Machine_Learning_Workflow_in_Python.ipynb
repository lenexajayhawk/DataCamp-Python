{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to teach us 4 superpowers.\n",
    "\n",
    "First, reviewing the basics. Mostly focused on classification in this course, where the class is denoted by y and the features denoted by X. Most classifiers expect numeric features. Need to convert string features to numbers, also known as feature engineering. Preprocessing is typically done with the LabelEncoder class from sklearn.preprocessing, using the fit_transform method. Model fitting uses .fit(features, label) (optimizing the model) and .predict(features) to develop the model and then get the results against unknown data. Model selection is using a variety of algorithms to develop models for better accuracy. Performance assessment is the process of using more data to increase our confidence in the model results. Using accuracy_score from sklearn.metrics is one way of assessing the accuracy of a model, though we have to be careful! Base accuracy is a misleading accuracy measure because assessing the data used to train the classifier produces bias. This phenomenon is known as overfitting. This is where the train_test_split function from sklearn.model_selection comes in to set aside training and test data. (ASIDE: In deep learning (neural nets) this is the optimal method of splitting data.) Train on X_train and y_train while testing on X_test to compare to y_test. \n",
    "\n",
    "Data loading -> Feature engineering -> Data splitting -> Model selection -> Pick best-performing model\n",
    "\n",
    "If only things were this easy!\n",
    "\n",
    "The four superpowers:\n",
    "\n",
    "1) Scaleable ways to tune your pipeline\n",
    "2) Making sure your predictions are relevant by involving domain experts\n",
    "3) Making sure your model performs well over time\n",
    "4) Fitting models when you don't have enough labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering\n",
    "\n",
    "You are tasked to predict whether a new cohort of loan applicants are likely to default on their loans. You have a historical dataset and wish to train a classifier on it. You notice that many features are in string format, which is a problem for your classifiers. You hence decide to encode the string columns numerically using LabelEncoder(). The function has been preloaded for you from the preprocessing submodule of sklearn. The dataset credit is also preloaded, as is a list of all column names whose data types are string, stored in non_numeric_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few lines of your data using head()\n",
    "credit.head(3)\n",
    "\n",
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])\n",
    "\n",
    "# Inspect the data types of the columns of the data frame\n",
    "print(credit.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first pipeline\n",
    "\n",
    "Your colleague has used AdaBoostClassifier for the credit scoring dataset. You want to also try out a random forest classifier. In this exercise, you will fit this classifier to the data and compare it to AdaBoostClassifier. Make sure to use train/test data splitting to avoid overfitting. The data is preloaded and transformed so that all features are numeric. The features are available as X and the labels as y. The module RandomForestClassifier has also been preloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create a random forest classifier, fixing the seed to 2\n",
    "rf_model = RandomForestClassifier(random_state=2).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Use it to predict the labels of the test data\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Assess the accuracy of both classifiers\n",
    "accuracies['rf'] = accuracy_score(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How complex should a model be? This is one of the most critical decisions a data scientist must make. Complexity is often related to the optional model parameters that can be set. The example uses Random Forest and the max_depth parameter, running one example set at 2 and another at 4 (max_depth = 2 and max_depth = 4). Individual trees can be viewed via the private .estimators_[n] attribute. Data should be split into training and test, but consider a hold-out or validation set of fresh data to run against the final version of the classifier. Or, use cross-validation to split the data into chunks that hold out a certain percentage of the data to be used as the test set. This is done n number of times depending on how many sets of sub-data are to be used in the cross-validation. Cross-validation is implemented as cross_val_score from sklearn.model_selection. Cross_val_score splits data into 3 groups by default.\n",
    "\n",
    "Model tuning to manage complexity can be done with grid search using GridSearchCV from sklearn.model_selection. Define a dictionary typically called param_grid with the values of the max_depth, for example, for the Random Forest classifier. Call GridSearchCV(RandomForestClassifier(), grid_search) and then grid.fit(X, y) to execute. The ._best_params attribute will tell us which max_depth value was optimal.\n",
    "\n",
    "In-sample accuracy is based on the training data. In-sample accuracy with the increasing max_depth values increases until it basically memorizes the structure of the training data, ultimately resulting in almost 100% accuracy. Out-of-sample accuracy is based on the training data and its results are much more realistic, never coming close to the 100% accuracy barrier that the in-sample assessment does, no matter how many values the max_depth is set to. In this case, once max_depth is > 10 the accuracy drops, which indicates overfitting!\n",
    "\n",
    "Fascinating analogy: If you memorized the study guide for a test then the test would have to be exactly the same as the study guide material or you'd be in a lot of trouble!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search CV for model complexity\n",
    "\n",
    "In the last slide, you saw how most classifiers have one or more hyperparameters that control its complexity. You also learned to tune them using GridSearchCV(). In this exercise, you will perfect this skill. You will experiment with:\n",
    "\n",
    "The number of trees, n_estimators, in a RandomForestClassifier.\n",
    "The maximum depth, max_depth, of the decision trees used in an AdaBoostClassifier.\n",
    "The number of nearest neighbors, n_neighbors, in KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid for n_estimators ranging from 1 to 10\n",
    "param_grid = {'n_estimators': range(1, 11)}\n",
    "\n",
    "# Optimize for a AdaBoostClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_\n",
    "\n",
    "# Define a grid for n_neighbors with values 10, 50 and 100\n",
    "param_grid = {'n_neighbors': [10, 50, 100]}\n",
    "\n",
    "# Optimize for KNeighborsClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering and overfitting\n",
    "\n",
    "Although adding features can increase performance it can also increase the risk of overfitting. \n",
    "\n",
    "Sometimes data does not come in nice clean tabular formats; ex: ECGs from multiple patients in variable time series lengths. The presented data is ECGs by patient with weird names that are looking for arrythymia, or unusual heart rates.\n",
    "\n",
    "Non-numeric data can be converted with LabelEnconder, but sometimes this isn't helpful because tree-based algorithms split the values which can result in a misunderstanding that they are in some order, which they might not be. This is where one-hot encoding comes to the rescue with the get_dummies method.\n",
    "\n",
    "You can also count keywords in categorical variables with CountVectorizer() from sklearn.feature_extraction.text, such as the number of times the word 'buy' is used in the purpose category for the loans data.\n",
    "\n",
    "The key point is how many features is too many features? With LabelEncoder a categorical column is converted into ordered numeric values and we don't add any features. With one-hot encoding we might go from 1 feature to 10 and with keyword splitting we could go to 15. In the arrythmias data there are 250 to begin with. This is known as increasing dimensionality.\n",
    "\n",
    "With more columns the algorithm has more opportunity to mistake coincidental patterns and patterns to real signals within the data. We can test this by adding columns to the algorithm that are purely random and watch as the training accuracy improves while the test accuracy declines. This is because the algorithm suddenly finds some relationship in the many variables that may not really exist and is able to memorize this pattern, while the new unseen test data does not follow that pattern and thus the algorithm can't find the same pattern.\n",
    "\n",
    "Feature Selection\n",
    "\n",
    "The SelectKBest from sklearn.feature_selection is one way to optimize the number of valuable attributes.\n",
    "\n",
    "There are tradeoffs in every decision made with regard to feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical encodings\n",
    "\n",
    "Your colleague has converted the columns in the credit dataset to numeric values using LabelEncoder(). He left one out: credit_history, which records the credit history of the applicant. You want to create two versions of the dataset. One will use LabelEncoder() and another one-hot encoding, for comparison purposes. The feature matrix is available to you as credit. You have LabelEncoder() preloaded and pandas as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(credit['credit_history'])\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], axis = 'columns')\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])], axis = 'columns')\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames to confirm that one-hot encoding results in more!\n",
    "X_hot.shape[1] > X_num.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature transformations\n",
    "\n",
    "You are discussing the credit dataset with the bank manager. She suggests that the safest loan applications tend to request mid-range credit amounts. Values that are either too low or too high suggest high risk. This means that a non-linear relationship might exist between this variable and the class. You want to test this hypothesis. You will construct a non-linear transformation of the feature. Then, you will assess which of the two features is better at predicting the class using SelectKBest() and the chi2() metric, both of which have been preloaded.\n",
    "\n",
    "The data is available as a pandas DataFrame called credit, with the class contained in the column class. You also have preloaded pandas as pd and numpy as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x-np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['diff'] = abs_diff(credit['credit_amount'])\n",
    "\n",
    "# Create a feature selector with chi2 that picks one feature\n",
    "sk = SelectKBest(chi2, k=1)\n",
    "\n",
    "# Use the selector to pick between credit_amount and diff\n",
    "sk.fit(credit[['credit_amount', 'diff']], credit['class'])\n",
    "\n",
    "# Inspect the results; credit_amount is the more valuable variable!\n",
    "sk.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "You just joined an arrhythmia detection startup and want to train a model on the arrhythmias dataset arrh. You noticed that random forests tend to win quite a few Kaggle competitions, so you want to try that out with a maximum depth of 2, 5, or 10, using grid search. You also observe that the dimension of the dataset is quite high so you wish to consider the effect of a feature selection method.\n",
    "\n",
    "To make sure you don't overfit by mistake, you have already split your data. You will use X_train and y_train for the grid search, and X_test and y_test to decide if feature selection helps. All four dataset folds are preloaded in your environment. You also have access to GridSearchCV(), train_test_split(), SelectKBest(), chi2() and RandomForestClassifier as rfc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value for max_depth among values 2, 5 and 10\n",
    "grid_search = GridSearchCV(\n",
    "  rfc(random_state=1), param_grid={'max_depth': [2, 5, 10]})\n",
    "best_value = grid_search.fit(\n",
    "  X_train, y_train).best_params_['max_depth']\n",
    "\n",
    "# Using the best value from above, fit a random forest\n",
    "clf = rfc(\n",
    "  random_state=1, max_depth=best_value).fit(X_train, y_train)\n",
    "\n",
    "# Apply SelectKBest with chi2 and pick top 100 features\n",
    "vt = SelectKBest(chi2, k=100).fit(X_train, y_train)\n",
    "\n",
    "# Create a new dataset only containing the selected features\n",
    "X_train_reduced = vt.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Fusion\n",
    "\n",
    "Sometimes a data set is not ready for modeling. Sometimes prepping the data by combining from multiple data sources is required. The tutorial then goes into how computer talk to each other via ports, sending packets of data back and forth. Flows are sessions of continuous data transfer between a port on a source computer and port on a destination computer based on some security protocol.\n",
    "\n",
    "This example is going to review potential cyberattacks by examining the LANL dataset, which describes the data exchanges between sending and receiving computers at a computer level. When we groupby the LANL data by computer we return an iterator; to access individual items within an iterator you have to call list(iterator_name)[n].\n",
    "\n",
    "Is the source or the destination bad?\n",
    "\n",
    "In the previous lesson, you used the destination computer as your entity of interest. However, your cybersecurity analyst just told you that it is the infected machines that generate the bad traffic, and will therefore appear as a source, not a destination, in the flows dataset.\n",
    "\n",
    "The data flows has been preloaded, as well as the list bad of infected IDs and the feature extractor featurizer() from the previous lesson. You also have numpy available as np, AdaBoostClassifier(), and cross_val_score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by source computer, and apply the feature extractor\n",
    "out = flows.groupby('source_computer').apply(featurize)\n",
    "\n",
    "# Convert the iterator to a dataframe by calling list on it\n",
    "X = pd.DataFrame(list(out), index=out.index)\n",
    "\n",
    "# Check which sources in X.index are bad to create labels\n",
    "y = [x in bads for x in X.index]\n",
    "\n",
    "# Report the average accuracy of Adaboost over 3-fold CV\n",
    "print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering on grouped data\n",
    "\n",
    "You will now build on the previous exercise, by considering one additional feature: the number of unique protocols used by each source computer. Note that with grouped data, it is always possible to construct features in this manner: you can take the number of unique elements of all categorical columns, and the mean of all numeric columns as your starting point. As before, you have flows preloaded, cross_val_score() for measuring accuracy, AdaBoostClassifier(), pandas as pd and numpy as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature counting unique protocols per source\n",
    "protocols = flows.groupby('source_computer').apply(\n",
    "  lambda df: len(set(df['protocol'])))\n",
    "\n",
    "# Convert this feature into a dataframe, naming the column\n",
    "protocols_DF = pd.DataFrame(\n",
    "  protocols, index=protocols.index, columns=['protocol'])\n",
    "\n",
    "# Now concatenate this feature with the previous dataset, X\n",
    "X_more = pd.concat([X, protocols_DF], axis=1)\n",
    "\n",
    "# Refit the classifier and report its accuracy\n",
    "print(np.mean(cross_val_score(\n",
    "  AdaBoostClassifier(), X_more, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning a heuristic into a classifier\n",
    "\n",
    "You are surprised by the fact that heuristics can be so helpful. So you decide to treat the heuristic that \"too many unique ports is suspicious\" as a classifier in its own right. You achieve that by thresholding the number of unique ports per source by the average number used in bad source computers -- these are computers for which the label is True. The dataset is preloaded and split into training and test, so you have objects X_train, X_test, y_train and y_test in memory. Your imports include accuracy_score(), and numpy as np. To clarify: you won't be fitting a classifier from scikit-learn in this exercise, but instead you will define your own classification rule explicitly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset X_train_bad by subselecting bad hosts\n",
    "X_train_bad = X_train[y_train]\n",
    "\n",
    "# Calculate the average of unique_ports in bad examples\n",
    "avg_bad_ports = np.mean(X_train_bad['unique_ports'])\n",
    "\n",
    "# Label as positive sources that use more ports than that\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "\n",
    "# Print the accuracy of the heuristic\n",
    "print(accuracy_score(y_test, pred_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining heuristics\n",
    "\n",
    "A different cyber analyst tells you that during certain types of attack, the infected source computer sends small bits of traffic, to avoid detection. This makes you wonder whether it would be better to create a combined heuristic that simultaneously looks for large numbers of ports and small packet sizes. Does this improve performance over the simple port heuristic? As with the last exercise, you have X_train, X_test, y_train and y_test in memory. The sample code also helps you reproduce the outcome of the port heuristic, pred_port. You also have numpy as np and accuracy_score() preloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of average_packet for bad sources\n",
    "avg_bad_packet = np.mean(X_train[y_train]['average_packet'])\n",
    "\n",
    "# Label as positive if average_packet is lower than that\n",
    "pred_packet = X_test['average_packet'] < avg_bad_packet\n",
    "\n",
    "# Find indices where pred_port and pred_packet both True\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "pred_both = pred_packet * pred_port\n",
    "\n",
    "# Ports only produced an accuracy of 0.919. Is this better?\n",
    "print(accuracy_score(y_test, pred_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with label noise\n",
    "\n",
    "One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as \"noisy\". Thankfully you know how to do that, using weighted learning. The contaminated data is available in your workspace as X_train, X_test, y_train_noisy, y_test. You want to see if you can improve the performance of a GaussianNB() classifier using weighted learning. You can use the optional parameter sample_weight, which is supported by the .fit() methods of most popular classifiers. The function accuracy_score() is preloaded. You can consult the image below for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian Naive Bayes classifier to the training data\n",
    "clf = GaussianNB().fit(X_train, y_train_noisy)\n",
    "\n",
    "# Report its accuracy on the test data\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Assign half the weight to the first 100 noisy examples\n",
    "weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)\n",
    "\n",
    "# Refit using weights and report accuracy. Has it improved?\n",
    "clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)\n",
    "print(accuracy_score(y_test, clf_weights.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions\n",
    "\n",
    "So far we've focused on accuracy, the total number of correct predictions, to measure classifier performance. \n",
    "\n",
    "Reminder of performance metrics\n",
    "\n",
    "Remember the credit dataset? With all the extra knowledge you now have about metrics, let's have another look at how good a random forest is on this dataset. You have already trained your classifier and obtained your confusion matrix on the test data. The test data and the results are available to you as tp, fp, fn and tn, for true positives, false positives, false negatives, and true negatives respectively. You also have the ground truth labels for the test data, y_test and the predicted labels, preds. The functions f1_score() and precision_score() have also been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, preds))\n",
    "\n",
    "print(precision_score(y_test, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world cost analysis\n",
    "\n",
    "You will still work on the credit dataset for this exercise. Recall that a \"positive\" in this dataset means \"bad credit\", i.e., a customer who defaulted on their loan, and a \"negative\" means a customer who continued to pay without problems. The bank manager informed you that the bank makes 10K profit on average from each \"good risk\" customer, but loses 150K from each \"bad risk\" customer. Your algorithm will be used to screen applicants, so those that are labeled as \"negative\" will be given a loan, and the \"positive\" ones will be turned down. What is the total cost of your classifier? The data is available as X_train, X_test, y_train and y_test. The functions confusion_matrix(), f1_score(), and precision_score() and RandomForestClassifier() are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest classifier to the training data\n",
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "\n",
    "# Label the test data\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Get false positives/negatives from the confusion matrix\n",
    "tp, fp, fn, tn = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# Now compute the cost using the manager's advice\n",
    "cost = fp*10 + fn*150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions part 2\n",
    "\n",
    "Instead of using the predict method, use the predict_proba method with test data to get probabilities for each prediction. This returns two values for each prediction, the first is the negative class and the 2nd is the positive class. Use list comprehension to set class values; for example s[1] > 0.5 for s in scores == clf.predict(X_test). This is an early way of getting to the Receiver Operating Curve (ROC), which is plotted with the True Positive Rate and the False Positive Rate.\n",
    "\n",
    "Where the ROC for one model is higher than another, it proves the superiority of that algorithm's result. \n",
    "\n",
    "The AUC (Area Under the Curve) is calculated with the roc_auc_score. The higher this score the better, where .5 would be no better than random. \n",
    "\n",
    "Default thresholding\n",
    "\n",
    "You would like to confirm that the DecisionTreeClassifier() uses the same default classification threshold as mentioned in the previous lesson, namely 0.5. It seems strange to you that all classifiers should use the same threshold. Let's check! A fitted decision tree classifier clf has been preloaded for you, as have the training and test data with their usual names: X_train, X_test, y_train and y_test. You will have to extract probability scores from the classifier using the .predict_proba() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the test data using the given classifier\n",
    "scores = clf.predict_proba(X_test)\n",
    "\n",
    "# Get labels from the scores using the default threshold\n",
    "preds = [s[1] > 0.5 for s in scores]\n",
    "\n",
    "# Use the predict method to label the test data again\n",
    "preds_default = clf.predict(X_test)\n",
    "\n",
    "# Compare the two sets of predictions\n",
    "all(preds == preds_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the threshold\n",
    "\n",
    "You heard that the default value of 0.5 maximizes accuracy in theory, but you want to test what happens in practice. So you try out a number of different threshold values, to see what accuracy you get, and hence determine the best-performing threshold value. You repeat this experiment for the F1 score. Is 0.5 the optimal threshold? Is the optimal threshold for accuracy and for the F1 score the same? Go ahead and find out! You have a scores matrix available, obtained by scoring the test data. The ground truth labels for the test data is also available as y_test. Finally, two numpy functions are preloaded, argmin() and argmax(), which retrieve the index of the minimum and maximum values in an array respectively, in addition to the metrics accuracy_score() and f1_score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of equally spaced threshold values\n",
    "t_range = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Store the predicted labels for each value of the threshold\n",
    "preds = [[s[1] > thr for s in scores] for thr in t_range]\n",
    "\n",
    "# Compute the accuracy for each threshold\n",
    "accuracies = [accuracy_score(y_test, p) for p in preds]\n",
    "\n",
    "# Compute the F1 score for each threshold\n",
    "f1_scores = [f1_score(y_test, p) for p in preds]\n",
    "\n",
    "# Report the optimal threshold for accuracy, and for F1\n",
    "print(t_range[argmax(accuracies)], t_range[argmax(f1_scores)])\n",
    "\n",
    "# The result of 0.5 and 0.25 indicates that the baseline accuracy is set to 0.5 but that\n",
    "# optimizing against another metric, such as F1 score, can actually find a new threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "One of the engineers in your arrhythmia detection startup rushes into your office to let you know that there is a problem with the ECG sensor for overweight users. You decide to reduce the influence of examples with weight over 80 by 50%. You are also told that since your startup is targeting the fitness market and makes no medical claims, scaring an athlete unnecessarily is costlier than missing a possible case of arrhythmia. You decide to create a custom loss that makes each \"false alarm\" ten times costlier than missing a case of arrhythmia. Does down-weighting overweight subjects improve this custom loss? Your training data X_train, y_train and test data X_test, y_test are preloaded, as are confusion_matrix(), numpy as np, and DecisionTreeClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scorer assigning more cost to false positives\n",
    "# False positives are 10x more painful than false negatives!\n",
    "def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp*fp + cost_fn*fn\n",
    "\n",
    "# Fit a DecisionTreeClassifier to the data and compute the loss\n",
    "clf = DecisionTreeClassifier(random_state=2).fit(X_train, y_train)\n",
    "print(my_scorer(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Refit, downweighting subjects whose weight is above 80\n",
    "weights = [0.5 if w > 80 else 1.0 for w in X_train.weight]\n",
    "clf_weighted = DecisionTreeClassifier().fit(\n",
    "  X_train, y_train, sample_weight=weights)\n",
    "print(my_scorer(y_test, clf_weighted.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first pipeline - again!\n",
    "\n",
    "Back in the arrhythmia startup, your monthly review is coming up, and as part of that an expert Python programmer will be reviewing your code. You decide to tidy up by following best practices and replace your script for feature selection and random forest classification, with a pipeline. You are using a training dataset available as X_train and y_train, and a number of modules: RandomForestClassifier, SelectKBest() and f_classif() for feature selection, as well as GridSearchCV and Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10, 20],\n",
    "   'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialize the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom scorers in pipelines\n",
    "\n",
    "You are proud of the improvement in your code quality, but just remembered that previously you had to use a custom scoring metric in order to account for the fact that false positives are costlier to your startup than false negatives. You hence want to equip your pipeline with scorers other than accuracy, including roc_auc_score(), f1_score(), and you own custom scoring function. The pipeline from the previous lesson is available as pipe, as is the parameter grid as params and the training data as X_train, y_train. You also have confusion_matrix() for the purpose of writing your own metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)\n",
    "\n",
    "# Now use a scoring function we created outselves\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(my_metric)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickles\n",
    "\n",
    "Finally, it is time for you to push your first model to production. It is a random forest classifier which you will use as a baseline, while you are still working to develop a better alternative. You have access to the data split in training test with their usual names, X_train, X_test, y_train and y_test, as well as to the modules RandomForestClassifier() and pickle, whose methods .load() and .dump() you will need for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Now load the model from file in the production environment\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function transformers in pipelines\n",
    "\n",
    "At some point, you were told that the sensors might be performing poorly for obese individuals. Previously you had dealt with that using weights, but now you are thinking that this information might also be useful for feature engineering, so you decide to replace the recorded weight of an individual with an indicator of whether they are obese. You want to do this using pipelines. You have numpy available as np, RandomForestClassifier(), FunctionTransformer(), and GridSearchCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "  Z = X.copy()\n",
    "  Z[:,1] = Z[:, 1] > multiplier*np.mean(Z[:,1])\n",
    "  return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier': [1, 2, 3]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge the champion\n",
    "\n",
    "Having pushed your random forest to production, you suddenly worry that a naive Bayes classifier might be better. You want to run a champion-challenger test, by comparing a naive Bayes, acting as the challenger, to exactly the model which is currently in production, which you will load from file to make sure there is no confusion. You will use the F1 score for assessment. You have the data X_train, X_test, y_train and y_test available as before and GaussianNB(), f1_score() and pickle()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the current model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data\n",
    "challenger = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation statistics\n",
    "\n",
    "You used grid search CV to tune your random forest classifier, and now want to inspect the cross-validation results to ensure you did not overfit. In particular you would like to take the difference of the mean test score for each fold from the mean training score. The dataset is available as X_train and y_train, the pipeline as pipe, and a number of modules are pre-loaded including pandas as pd and GridSearchCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your pipeline using GridSearchCV with three folds\n",
    "grid_search = GridSearchCV(\n",
    "  pipe, params, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit the grid search\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Store the results of CV into a pandas dataframe\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "# Print the difference between mean test and training scores\n",
    "print(results['mean_test_score']-results['mean_train_score'])\n",
    "# RESULTS: -0.2345, -0.2463, -0.3082, -0.2655 indicating potential overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the window size\n",
    "\n",
    "Tuning window size can be important when time frames change in the training data, and windows can be used to determine if the data has changed enough to need algorithm re-training.\n",
    "\n",
    "You want to check for yourself that the optimal window size for the arrhythmia dataset is 50. You have been given the dataset as a pandas data frame called arrh, and want to use a subset of the data up to time t_now. Your test data is available as X_test, y_test. You will try out a number of window sizes, ranging from 10 to 100, fit a naive Bayes classifier to each window, assess its F1 score on the test data, and then pick the best performing window size. You also have numpy available as np, and the function f1_score() has been imported already. Finally, an empty list called accuracies has been initialized for you to store the accuracies of the windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now-w_size+1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', axis = 1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]\n",
    "\n",
    "# Well done! You now realise that the possibility of dataset shift introduces yet another parameter to optimize: \n",
    "# the window size. This cannot be done with Cross-Validation on historical data, but instead requires the technique \n",
    "# shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "You have two concerns about your pipeline at the arrhythmia detection startup:\n",
    "\n",
    "The app was trained on patients of all ages, but is primarily being used by fitness users who tend to be young. You suspect this might be a case of domain shift, and hence want to disregard all examples above 50 years old.\n",
    "You are still concerned about overfitting, so you want to see if making the random forest classifier less complex and selecting some features might help with that.\n",
    "\n",
    "You will create a pipeline with a feature selection SelectKBest() step and a RandomForestClassifier, both of which have been imported. You also have access to GridSearchCV(), Pipeline, numpy as np and pickle. The data is available as arrh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "  ('ft', SelectKBest()), ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = {'ft__k':[5, 10], 'clf__max_depth':[10, 20]}\n",
    "\n",
    "# Execute grid search CV on a dataset containing under 50s\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid)\n",
    "arrh = arrh.iloc[np.where(arrh['age'] < 50)]\n",
    "grid_search.fit(arrh.drop('class', 1), arrh['class'])\n",
    "\n",
    "# Push the fitted pipeline to production\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple outlier\n",
    "\n",
    "When you first encounter a new type of algorithm, it is always a great idea to test it with a very simple example. So you decide to create a list containing thirty examples with the value 1.0 and just one example with value 10.0, which you expect should be flagged as an outlier. To make sure you use the algorithm correctly, you convert the list to a pandas dataframe, and feed it into the local outlier factor algorithm. pandas is available to you as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LocalOutlierFactor module\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained\n",
    "x = [1]*30\n",
    "x.append(10)\n",
    "\n",
    "# Cast to a data frame\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X)) # Came back with a list of 30 1s and 1 -1, so the single outlier was identified!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoF (Local Outlier Factor) contamination\n",
    "\n",
    "Your medical advisor at the arrhythmia startup informs you that your training data might not contain all possible types of arrhythmia. How on earth will you detect these other types without any labeled examples? Could an anomaly detector tell the difference between healthy and unhealthy without access to labels? But first, you experiment with the contamination parameter to see its effect on the confusion matrix. You have LocalOutlierFactor as lof, numpy as np, the labels as ground_truth encoded in -1 and 1 just like local outlier factor output, and the unlabeled training data as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fit the local outlier factor and output predictions\n",
    "preds = lof().fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))\n",
    "\n",
    "# 2) Try it again!\n",
    "# Set the contamination parameter to 0.2\n",
    "preds = lof(contamination=0.2).fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))\n",
    "\n",
    "# 3) Change the outlier frequency again\n",
    "# Contamination to match outlier frequency in ground_truth\n",
    "preds = lof(\n",
    "  contamination=np.mean(ground_truth == -1.0)).fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novelty detection\n",
    "\n",
    "A simple novelty\n",
    "\n",
    "You find novelty detection more useful than outlier detection, but want to make sure it works on the simple example you came up with before. This time you will use a sequence of thirty examples all with value 1.0 as a training set, and try to see if the example 10.0 is labeled as a novelty. You have access to pandas as pd, and the LocalOutlierFactor module as lof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of thirty 1s and cast to a dataframe\n",
    "X = pd.DataFrame(list([1.0]*30))\n",
    "\n",
    "# Create an instance of a lof novelty detector\n",
    "detector = lof(novelty=True)\n",
    "\n",
    "# Fit the detector to the data\n",
    "detector.fit(X)\n",
    "\n",
    "# Use it to predict the label of an example with value 10.0\n",
    "print(detector.predict(pd.DataFrame([10.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three novelty detectors\n",
    "\n",
    "Finally, you know enough to run some tests on the use of a few anomaly detectors on the arrhythmia dataset. To test their performance, you will train them on an unlabeled training dataset, but then compare their predictions to the ground truth on the test data using their method .score_samples(). This time, you will be asked to import the detectors as part of the exercise, but you do have the data X_train, X_test, y_train, y_test preloaded as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the novelty detector\n",
    "from sklearn.svm import OneClassSVM as onesvm\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "svm_detector = onesvm().fit(X_train)\n",
    "scores = svm_detector.score_samples(X_test)\n",
    "\n",
    "# Now use the IsolationForest\n",
    "from sklearn.ensemble import IsolationForest as isof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "isof_detector = isof().fit(X_train)\n",
    "scores = isof_detector.score_samples(X_test)\n",
    "\n",
    "# Use LoF as a novelty detector this time\n",
    "# Import the novelty detector\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "lof_detector = lof(novelty=True).fit(X_train)\n",
    "scores = lof_detector.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
